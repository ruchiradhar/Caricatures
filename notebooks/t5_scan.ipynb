{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9tm-b0EO91i"
   },
   "source": [
    "#### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  4 05:28:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   24C    P8              22W / 350W |      3MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lPKvlBsGNWir"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ohHRw4qPCEG"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "zpow5E2INhCz"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, get_scheduler,\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALja22i8PhZI"
   },
   "source": [
    "#### Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "oX1EQWOjPjZa"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "dataset_name = 'scan'\n",
    "dataset_config_name = 'simple'\n",
    "trust_remote_code = True\n",
    "preprocessing_num_workers = None\n",
    "overwrite_cache = False\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "model_name = 'google-t5/t5-base'\n",
    "#model_name = 'google/flan-t5-base'\n",
    "# gpt2\n",
    "# gemma\n",
    "\n",
    "source_prefix = \"\"\n",
    "max_target_length = 1024\n",
    "max_source_length = 1024\n",
    "padding = False\n",
    "ignore_pad_token_for_loss = True\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "\n",
    "weight_decay = 0.0\n",
    "learning_rate = 5e-5\n",
    "train_steps = 100000\n",
    "eval_steps = 2000\n",
    "lr_scheduler_type = 'linear'\n",
    "num_warmup_steps = 0\n",
    "checkpointing_steps = None\n",
    "num_beams = 1\n",
    "\n",
    "output_dir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "597IKDnFPbfw"
   },
   "source": [
    "#### Setup accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "BedTb-n8Peb1"
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHPq-Ri-P7Sc"
   },
   "source": [
    "#### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "BhvACU4bP8ie"
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(dataset_name, dataset_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvYzpq_mSB6L"
   },
   "source": [
    "#### Split train set into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "lDFWJlGBRWOT"
   },
   "outputs": [],
   "source": [
    "train_val_split = raw_datasets['train'].train_test_split(test_size=0.1, seed=SEED)\n",
    "raw_datasets['train'] = train_val_split['train']\n",
    "raw_datasets['validation'] = train_val_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "424Yjdc3SJlT"
   },
   "source": [
    "#### Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "mQNMd8oSRcB1",
    "outputId": "77d55159-428a-4cad-de84-7afd1c4e20a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config, trust_remote_code=trust_remote_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb9NYcK4Tca5"
   },
   "source": [
    "#### Resize the embeddings when necessary to avoid index errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "Ut_d43a9TSNB"
   },
   "outputs": [],
   "source": [
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "  model.resize_token_embeddings(len(tokenizer))\n",
    "if model.config.decoder_start_token_id is None:\n",
    "  raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "prefix = source_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4ZUPMPtT5rS"
   },
   "source": [
    "#### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "FI0nux9dT7DO"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c851da980f14a9ca3eeaf8b897759b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "input_column = column_names[0]\n",
    "output_column = column_names[1]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "  inputs = examples[input_column]\n",
    "  targets = examples[output_column]\n",
    "  inputs = [prefix + inp for inp in inputs]\n",
    "  model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "  # tokenize targets with the `text_target` keyword argument\n",
    "  labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "  # if we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "  # padding in the loss.\n",
    "  if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "  train_dataset = raw_datasets[\"train\"].map(\n",
    "      preprocess_function,\n",
    "      batched=True,\n",
    "      num_proc=preprocessing_num_workers,\n",
    "      remove_columns=column_names,\n",
    "      load_from_cache_file=not overwrite_cache,\n",
    "      desc=\"Running tokenizer on dataset\",\n",
    "  )\n",
    "\n",
    "  eval_dataset = raw_datasets[\"validation\"].map(\n",
    "      preprocess_function,\n",
    "      batched=True,\n",
    "      num_proc=preprocessing_num_workers,\n",
    "      remove_columns=column_names,\n",
    "      load_from_cache_file=not overwrite_cache,\n",
    "      desc=\"Running tokenizer on dataset\",\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcRg5UddwIf5"
   },
   "source": [
    "#### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muxsraWMtg-5",
    "outputId": "b68cc178-fa13-4a6b-fda6-4d6192342827"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:595: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "label_pad_token_id = -100 if ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHosYi-xxz42"
   },
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "cf8xI0Aox04t"
   },
   "outputs": [],
   "source": [
    "# split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ftRwYq_yF-7"
   },
   "source": [
    "#### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "JvlP-JMryG0v"
   },
   "outputs": [],
   "source": [
    "overrode_max_train_steps = False\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=train_steps * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap6VCdMNnAA8"
   },
   "source": [
    "#### Prepare everything with Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "FzJq-hetnDLX"
   },
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwKvZP-iz8nM"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688,
     "referenced_widgets": [
      "8d903c55226c4febbcf7238ee1fa12a0",
      "5a6cf86460524041bdd0e8edc9f90c58",
      "cf79941b6f22437cad4ab2625a4d46fe",
      "ee3c996e03f04aa9ab9757e31a9b52d1",
      "904a8bf5ebed47128c31c7fee45cfc9e",
      "767e9896d6144015a21a1e0d2590c699",
      "d78c41990b534c29b41b8a4e33a9a1dc",
      "29e4acdbf0bb4fa68010f4ae6c5bf5e3",
      "e18cc431bce04488a9408bfce7480cfb",
      "72acc0ec28bc4466b4de54f1878de51e",
      "99333d4d4f804e27b0f3338bef6d7156",
      "7cfd353743714fa6914a5f957103b07e",
      "55519b6832834a1fb3edb3c9eaa589eb",
      "1bbce7067cf34b9ab913d23612fe235f",
      "a248f331dc644d2e8793b2d720f4a0bd",
      "838b91d9541a4aa7a53743dee427a274",
      "46bd3d7d6a5245b19e57e189b5552a8a",
      "e081ca4cf08f48128969e2889461b5f9",
      "e82c62a7109d425a8a3797f94a9982a6",
      "1ed8bc7a39e140bda525ee93b807149e",
      "dbe51e5446d74bbd96a7de254c95488b",
      "a575f7622de647fb91cfb6edc6348d33"
     ]
    },
    "id": "LrVmlY0dz9kX",
    "outputId": "fbe9f6be-6c82-43db-a073-de880f731eb5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a6b84bdd914b92b807ce7fdd444020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01387f3eb6964972a8a67c5acefe41f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps : 2000\n",
      "accuracy : 0.328750747160789\n",
      "steps : 4000\n",
      "accuracy : 0.4632396891811118\n",
      "steps : 6000\n",
      "accuracy : 0.6192468619246861\n",
      "steps : 8000\n",
      "accuracy : 0.7112970711297071\n",
      "steps : 10000\n",
      "accuracy : 0.7423789599521817\n",
      "steps : 12000\n",
      "accuracy : 0.7232516437537359\n",
      "steps : 14000\n",
      "accuracy : 0.7573221757322176\n",
      "steps : 16000\n",
      "accuracy : 0.7387925881649731\n",
      "steps : 18000\n",
      "accuracy : 0.8105200239091452\n",
      "steps : 20000\n",
      "accuracy : 0.7979677226539151\n",
      "steps : 22000\n",
      "accuracy : 0.7746563060370592\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     13\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 14\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2125\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(train_steps), disable=not accelerator.is_local_main_process)\n",
    "eval_bar = tqdm(range(len(eval_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "total_loss = 0\n",
    "completed_steps = 0\n",
    "model.train()\n",
    "\n",
    "while True:\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    with accelerator.accumulate(model):\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      total_loss += loss.detach().float()\n",
    "      accelerator.backward(loss)\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # checks if the accelerator has performed an optimization step behind the scenes\n",
    "      if accelerator.sync_gradients:\n",
    "        progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "      if completed_steps % eval_steps == 0:\n",
    "        model.eval()\n",
    "\n",
    "        # metric\n",
    "        accuracy = 0.0\n",
    "\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": val_max_target_length,\n",
    "            \"num_beams\": num_beams,\n",
    "        }\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "              generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                  batch[\"input_ids\"],\n",
    "                  attention_mask=batch[\"attention_mask\"],\n",
    "                  **gen_kwargs,\n",
    "              )\n",
    "\n",
    "              generated_tokens = accelerator.pad_across_processes(\n",
    "                  generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "                )\n",
    "              labels = batch[\"labels\"]\n",
    "              # we did not pad to max length, we need to pad the labels too\n",
    "              labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "              generated_tokens, labels = accelerator.gather_for_metrics((generated_tokens, labels))\n",
    "              generated_tokens = generated_tokens.cpu().numpy()\n",
    "              labels = labels.cpu().numpy()\n",
    "\n",
    "              if ignore_pad_token_for_loss:\n",
    "                # replace -100 in the labels as we can't decode them.\n",
    "                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "              if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "              decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "              decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "              accuracy += sum([decoded_preds[i] == decoded_labels[i] for i in range(len(decoded_preds))])\n",
    "\n",
    "              eval_bar.update(1)\n",
    "\n",
    "        eval_bar.refresh()\n",
    "        eval_bar.reset()\n",
    "\n",
    "        accelerator.print('steps : {}'.format(completed_steps))\n",
    "        accelerator.print('accuracy : {}'.format(accuracy / len(raw_datasets['validation'])))\n",
    "        accelerator.print('')\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        # save\n",
    "        new_path = output_dir+'checkpoint_'+str(completed_steps)\n",
    "        if not os.path.isdir(new_path):\n",
    "          os.makedirs(new_path)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            new_path, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    if completed_steps == train_steps:\n",
    "      raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlPKWC91lMda"
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = AutoModelForSeq2SeqLM.from_pretrained(output_dir+'checkpoint_18000', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72920ce9f89c4fbe9d57831f6bc87d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/4182 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = raw_datasets[\"test\"].map(\n",
    "      preprocess_function,\n",
    "      batched=True,\n",
    "      num_proc=preprocessing_num_workers,\n",
    "      remove_columns=column_names,\n",
    "      load_from_cache_file=not overwrite_cache,\n",
    "      desc=\"Running tokenizer on dataset\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=per_device_eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model, test_dataloader = accelerator.prepare(test_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be506291de484da4afc69a39492304e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9791965566714491\n"
     ]
    }
   ],
   "source": [
    "test_bar = tqdm(range(len(test_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "# metric\n",
    "accuracy = 0.0\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"max_length\": max_target_length,\n",
    "    \"num_beams\": num_beams,\n",
    "}\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "      generated_tokens = accelerator.unwrap_model(test_model).generate(\n",
    "          batch[\"input_ids\"],\n",
    "          attention_mask=batch[\"attention_mask\"],\n",
    "          **gen_kwargs,\n",
    "      )\n",
    "\n",
    "      generated_tokens = accelerator.pad_across_processes(\n",
    "          generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "      labels = batch[\"labels\"]\n",
    "      # we did not pad to max length, we need to pad the labels too\n",
    "      labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "      generated_tokens, labels = accelerator.gather_for_metrics((generated_tokens, labels))\n",
    "      generated_tokens = generated_tokens.cpu().numpy()\n",
    "      labels = labels.cpu().numpy()\n",
    "\n",
    "      if ignore_pad_token_for_loss:\n",
    "        # replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "      if isinstance(generated_tokens, tuple):\n",
    "        generated_tokens = generated_tokens[0]\n",
    "      decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "      accuracy += sum([decoded_preds[i] == decoded_labels[i] for i in range(len(decoded_preds))])\n",
    "\n",
    "      test_bar.update(1)\n",
    "\n",
    "accelerator.print('accuracy : {}'.format(accuracy / len(raw_datasets['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18k accuracy : 0.9791965566714491\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = -1\n",
    "for sample in raw_datasets['train']:\n",
    "    model_inputs = tokenizer(sample['commands'], truncation=False)\n",
    "    l = len(model_inputs['input_ids'])\n",
    "    if l > maxlen:\n",
    "        maxlen = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = -1\n",
    "for sample in raw_datasets['train']:\n",
    "    model_outputs = tokenizer(sample['actions'], truncation=False)\n",
    "    l = len(model_outputs['input_ids'])\n",
    "    if l > maxlen:\n",
    "        maxlen = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1bbce7067cf34b9ab913d23612fe235f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e82c62a7109d425a8a3797f94a9982a6",
      "max": 419,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ed8bc7a39e140bda525ee93b807149e",
      "value": 119
     }
    },
    "1ed8bc7a39e140bda525ee93b807149e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29e4acdbf0bb4fa68010f4ae6c5bf5e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46bd3d7d6a5245b19e57e189b5552a8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55519b6832834a1fb3edb3c9eaa589eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46bd3d7d6a5245b19e57e189b5552a8a",
      "placeholder": "​",
      "style": "IPY_MODEL_e081ca4cf08f48128969e2889461b5f9",
      "value": " 28%"
     }
    },
    "5a6cf86460524041bdd0e8edc9f90c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_767e9896d6144015a21a1e0d2590c699",
      "placeholder": "​",
      "style": "IPY_MODEL_d78c41990b534c29b41b8a4e33a9a1dc",
      "value": "  1%"
     }
    },
    "72acc0ec28bc4466b4de54f1878de51e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "767e9896d6144015a21a1e0d2590c699": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cfd353743714fa6914a5f957103b07e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55519b6832834a1fb3edb3c9eaa589eb",
       "IPY_MODEL_1bbce7067cf34b9ab913d23612fe235f",
       "IPY_MODEL_a248f331dc644d2e8793b2d720f4a0bd"
      ],
      "layout": "IPY_MODEL_838b91d9541a4aa7a53743dee427a274"
     }
    },
    "838b91d9541a4aa7a53743dee427a274": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d903c55226c4febbcf7238ee1fa12a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a6cf86460524041bdd0e8edc9f90c58",
       "IPY_MODEL_cf79941b6f22437cad4ab2625a4d46fe",
       "IPY_MODEL_ee3c996e03f04aa9ab9757e31a9b52d1"
      ],
      "layout": "IPY_MODEL_904a8bf5ebed47128c31c7fee45cfc9e"
     }
    },
    "904a8bf5ebed47128c31c7fee45cfc9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99333d4d4f804e27b0f3338bef6d7156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a248f331dc644d2e8793b2d720f4a0bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbe51e5446d74bbd96a7de254c95488b",
      "placeholder": "​",
      "style": "IPY_MODEL_a575f7622de647fb91cfb6edc6348d33",
      "value": " 119/419 [06:03&lt;12:36,  2.52s/it]"
     }
    },
    "a575f7622de647fb91cfb6edc6348d33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf79941b6f22437cad4ab2625a4d46fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29e4acdbf0bb4fa68010f4ae6c5bf5e3",
      "max": 100000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e18cc431bce04488a9408bfce7480cfb",
      "value": 900
     }
    },
    "d78c41990b534c29b41b8a4e33a9a1dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbe51e5446d74bbd96a7de254c95488b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e081ca4cf08f48128969e2889461b5f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e18cc431bce04488a9408bfce7480cfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e82c62a7109d425a8a3797f94a9982a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee3c996e03f04aa9ab9757e31a9b52d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72acc0ec28bc4466b4de54f1878de51e",
      "placeholder": "​",
      "style": "IPY_MODEL_99333d4d4f804e27b0f3338bef6d7156",
      "value": " 900/100000 [2:30:33&lt;9:26:33,  2.92it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
